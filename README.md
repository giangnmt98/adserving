# MLOps Serve

M·ªôt h·ªá th·ªëng MLOps ho√†n ch·ªânh s·ª≠ d·ª•ng Ray Serve v√† MLflow ƒë·ªÉ tri·ªÉn khai v√† ph·ª•c v·ª• nhi·ªÅu m√¥ h√¨nh machine learning song song v·ªõi kh·∫£ nƒÉng zero-downtime deployment, auto-scaling, v√† monitoring to√†n di·ªán.

## üöÄ T√≠nh nƒÉng ch√≠nh

### Qu·∫£n l√Ω m√¥ h√¨nh (Model Management)
- **MLflow Model Registry Integration**: T·ª± ƒë·ªông qu·∫£n l√Ω m√¥ h√¨nh c√≥ tr·∫°ng th√°i "Production"
- **Zero-downtime Deployment**: C·∫≠p nh·∫≠t m√¥ h√¨nh m√† kh√¥ng gi√°n ƒëo·∫°n service
- **Model Caching**: Cache th√¥ng minh v·ªõi LRU eviction
- **Automatic Model Updates**: T·ª± ƒë·ªông ph√°t hi·ªán v√† load m√¥ h√¨nh m·ªõi

### Ray Serve Deployment
- **Parallel Model Serving**: Deploy nhi·ªÅu m√¥ h√¨nh song song
- **Auto-scaling**: T·ª± ƒë·ªông ƒëi·ªÅu ch·ªânh s·ªë l∆∞·ª£ng replicas theo t·∫£i
- **Resource Management**: Qu·∫£n l√Ω t√†i nguy√™n CPU/GPU hi·ªáu qu·∫£
- **Health Monitoring**: Ki·ªÉm tra s·ª©c kh·ªèe m√¥ h√¨nh li√™n t·ª•c

### Monitoring & Performance
- **Real-time Monitoring**: Gi√°m s√°t t√†i nguy√™n v√† hi·ªáu su·∫•t real-time
- **Performance Optimization**: T·ªëi ∆∞u batch size v√† scaling
- **System Health Score**: ƒê√°nh gi√° t·ªïng th·ªÉ s·ª©c kh·ªèe h·ªá th·ªëng
- **Detailed Metrics**: Thu th·∫≠p metrics chi ti·∫øt cho debugging

### Logging & Debugging
- **Structured Logging**: JSON logging v·ªõi metadata ƒë·∫ßy ƒë·ªß
- **Performance Tracking**: Theo d√µi th·ªùi gian x·ª≠ l√Ω t·ª´ng operation
- **Error Handling**: X·ª≠ l√Ω l·ªói to√†n di·ªán v·ªõi context
- **Debug Utilities**: C√¥ng c·ª• debugging v√† troubleshooting

## üìã Y√™u c·∫ßu h·ªá th·ªëng

- Python 3.8+
- MLflow Server (cho Model Registry)
- Ray Cluster (t√πy ch·ªçn, c√≥ th·ªÉ ch·∫°y local)
- 4GB+ RAM (khuy·∫øn ngh·ªã 8GB+)
- GPU (t√πy ch·ªçn, cho c√°c m√¥ h√¨nh y√™u c·∫ßu GPU)

## üõ†Ô∏è C√†i ƒë·∫∑t

### 1. Clone repository v√† c√†i ƒë·∫∑t dependencies

```bash
git clone <repository-url>
cd mlops-serve
pip install -r requirements.txt
```

### 2. C·∫•u h√¨nh MLflow Server

```bash
# Kh·ªüi ƒë·ªông MLflow server
mlflow server --host 0.0.0.0 --port 5000 --default-artifact-root ./mlruns
```

### 3. C·∫•u h√¨nh h·ªá th·ªëng

Sao ch√©p v√† ch·ªânh s·ª≠a file c·∫•u h√¨nh:

```bash
cp config.yaml my_config.yaml
# Ch·ªânh s·ª≠a my_config.yaml theo nhu c·∫ßu
```

## üöÄ S·ª≠ d·ª•ng

### Kh·ªüi ƒë·ªông server

```bash
# S·ª≠ d·ª•ng c·∫•u h√¨nh m·∫∑c ƒë·ªãnh
python -m mlops_serve.mlops_server

# S·ª≠ d·ª•ng file c·∫•u h√¨nh t√πy ch·ªânh
python -m mlops_serve.mlops_server --config my_config.yaml

# Ch·ªâ ƒë·ªãnh host v√† port
python -m mlops_serve.mlops_server --host 0.0.0.0 --port 8080
```

### S·ª≠ d·ª•ng trong code

```python
from mlops_serve import MLOpsServer, Config

# T·∫°o server v·ªõi c·∫•u h√¨nh m·∫∑c ƒë·ªãnh
server = MLOpsServer()

# Ho·∫∑c v·ªõi c·∫•u h√¨nh t√πy ch·ªânh
config = Config.from_file("my_config.yaml")
server = MLOpsServer(config)

# Ch·∫°y server
server.run()
```

## üìö API Documentation

Server cung c·∫•p REST API ƒë·∫ßy ƒë·ªß v·ªõi Swagger UI t·∫°i `http://localhost:8000/docs`

### Endpoints ch√≠nh

#### Health Check
```http
GET /health
```

#### Prediction
```http
POST /api/v1/predict
Content-Type: application/json

{
    "model_name": "my_model",
    "data": {"feature1": 1.0, "feature2": 2.0},
    "request_id": "optional_id"
}
```

#### Model Management
```http
# Li·ªát k√™ t·∫•t c·∫£ m√¥ h√¨nh
GET /api/v1/models

# Tr·∫°ng th√°i m√¥ h√¨nh c·ª• th·ªÉ
GET /api/v1/models/{model_name}/status

# Deploy m√¥ h√¨nh
POST /api/v1/models/{model_name}/deploy

# Undeploy m√¥ h√¨nh
DELETE /api/v1/models/{model_name}/deploy
```

#### System Monitoring
```http
# Tr·∫°ng th√°i h·ªá th·ªëng
GET /api/v1/system/status

# Metrics chi ti·∫øt
GET /api/v1/system/metrics

# C·∫•u h√¨nh h·ªá th·ªëng
GET /api/v1/system/config
```

## üîß C·∫•u h√¨nh

### C·∫•u h√¨nh c∆° b·∫£n

```yaml
# MLflow
mlflow:
  tracking_uri: "http://localhost:5000"

# API
api_host: "0.0.0.0"
api_port: 8000

# Auto-deployment
enable_auto_deployment: true
```

### C·∫•u h√¨nh Resource Management

```yaml
deployment:
  resource_config:
    num_cpus: 2.0
    num_gpus: 1.0
    memory: 2048  # MB
  
  autoscaling:
    min_replicas: 1
    max_replicas: 5
    target_num_ongoing_requests_per_replica: 2
```

### C·∫•u h√¨nh Monitoring

```yaml
monitoring:
  collection_interval: 10
  alert_thresholds:
    cpu_percent: 80.0
    memory_percent: 85.0
    error_rate: 0.01
```

### C·∫•u h√¨nh Security

```yaml
security:
  enable_auth: true
  api_key: "your-secret-key"
  allowed_origins: ["https://yourdomain.com"]
```

## üìä Monitoring Dashboard

H·ªá th·ªëng cung c·∫•p monitoring dashboard qua Ray Dashboard:
- Truy c·∫≠p: `http://localhost:8265`
- Xem resource usage, model performance, scaling metrics
- Real-time monitoring v√† alerting

## üîç Logging

### Log Files
- `logs/mlops_serve.log`: Main application logs
- `logs/errors.log`: Error logs only
- `logs/performance.log`: Performance metrics

### Log Format
```json
{
    "timestamp": 1640995200.0,
    "level": "INFO",
    "logger": "mlops_serve",
    "message": "Model deployed successfully",
    "extra": {
        "model_name": "my_model",
        "deployment_time": 2.5
    }
}
```

## üß™ Testing

### Unit Tests
```bash
pytest tests/
```

### API Testing
```bash
# Test prediction endpoint
curl -X POST "http://localhost:8000/api/v1/predict" \
     -H "Content-Type: application/json" \
     -d '{"model_name": "test_model", "data": {"x": 1.0}}'
```

### Load Testing
```python
import asyncio
import aiohttp

async def load_test():
    async with aiohttp.ClientSession() as session:
        tasks = []
        for i in range(100):
            task = session.post(
                "http://localhost:8000/api/v1/predict",
                json={"model_name": "test_model", "data": {"x": i}}
            )
            tasks.append(task)
        
        responses = await asyncio.gather(*tasks)
        print(f"Completed {len(responses)} requests")

asyncio.run(load_test())
```

## üö® Troubleshooting

### Common Issues

#### 1. MLflow Connection Error
```
Error: Cannot connect to MLflow server
```
**Solution**: Ki·ªÉm tra MLflow server ƒëang ch·∫°y v√† c·∫•u h√¨nh `tracking_uri` ƒë√∫ng.

#### 2. Ray Initialization Error
```
Error: Ray cluster not available
```
**Solution**: ƒê·∫£m b·∫£o Ray cluster ƒëang ch·∫°y ho·∫∑c s·ª≠ d·ª•ng local mode.

#### 3. Model Not Found
```
Error: Model not found in registry
```
**Solution**: Ki·ªÉm tra m√¥ h√¨nh ƒë√£ ƒë∆∞·ª£c register v·ªõi stage "Production" trong MLflow.

#### 4. Memory Issues
```
Error: Out of memory
```
**Solution**: Gi·∫£m `model_cache_size` ho·∫∑c tƒÉng memory cho system.

### Debug Mode

```bash
# Ch·∫°y v·ªõi debug logging
python -m mlops_serve.mlops_server --log-level DEBUG
```

### Health Checks

```bash
# Ki·ªÉm tra health c·ªßa h·ªá th·ªëng
curl http://localhost:8000/health

# Ki·ªÉm tra metrics
curl http://localhost:8000/api/v1/system/metrics
```

## üîÑ Zero-Downtime Deployment

H·ªá th·ªëng h·ªó tr·ª£ zero-downtime deployment:

1. **Model Update Detection**: T·ª± ƒë·ªông ph√°t hi·ªán m√¥ h√¨nh m·ªõi trong MLflow
2. **Graceful Replacement**: Load m√¥ h√¨nh m·ªõi song song v·ªõi m√¥ h√¨nh c≈©
3. **Traffic Switching**: Chuy·ªÉn traffic sang m√¥ h√¨nh m·ªõi khi s·∫µn s√†ng
4. **Rollback Support**: C√≥ th·ªÉ rollback n·∫øu m√¥ h√¨nh m·ªõi c√≥ v·∫•n ƒë·ªÅ

## üìà Performance Optimization

### Auto-scaling
- T·ª± ƒë·ªông scale up khi t·∫£i cao
- Scale down khi t·∫£i th·∫•p
- Configurable thresholds v√† metrics

### Batching
- Automatic request batching
- Optimal batch size detection
- Reduced inference overhead

### Caching
- Model caching v·ªõi LRU eviction
- Prediction result caching (t√πy ch·ªçn)
- Efficient memory management

## üîê Security

### Authentication
- API key authentication
- JWT token support (t√πy ch·ªçn)
- Role-based access control

### Network Security
- CORS configuration
- Rate limiting
- HTTPS support

### Data Security
- Input validation
- Secure logging (no sensitive data)
- Audit trails

## üìù Best Practices

### Model Management
1. S·ª≠ d·ª•ng semantic versioning cho models
2. Test models thoroughly tr∆∞·ªõc khi promote l√™n Production
3. Monitor model performance sau deployment
4. Implement model rollback strategy

### Resource Management
1. Set appropriate resource limits
2. Monitor resource usage
3. Use GPU efficiently
4. Implement proper cleanup

### Monitoring
1. Set up alerting cho critical metrics
2. Regular health checks
3. Log aggregation v√† analysis
4. Performance baseline tracking

## ü§ù Contributing

1. Fork repository
2. T·∫°o feature branch
3. Implement changes v·ªõi tests
4. Submit pull request

## üìÑ License

MIT License - xem file LICENSE ƒë·ªÉ bi·∫øt chi ti·∫øt.

## üÜò Support

- GitHub Issues: B√°o c√°o bugs v√† feature requests
- Documentation: Xem docs/ folder
- Examples: Xem examples/ folder

---

**MLOps Serve** - Production-ready ML model serving v·ªõi Ray Serve v√† MLflow üöÄ