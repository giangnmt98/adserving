# =============================================================================
# ANOMALY DETECTION SERVE - COMPLETE CONFIGURATION
# Enhanced configuration for serving hundreds of ML models with tier-based deployment
# =============================================================================

# 1. API Settings
api_host: "0.0.0.0"              # Bind tất cả interfaces (0.0.0.0 = all)
api_port: 8000                   # Port chính của FastAPI service
api_prefix: "/api/v1"            # URL prefix cho tất cả endpoints
api_version: "v1.0"              # Version hiển thị trong docs
enable_docs: true                # Bật Swagger/OpenAPI docs
enable_openapi: true             # Bật OpenAPI specification

# 2. Ray Cluster - Hạ tầng tính toán phân tán cho hundreds models
ray:
  address: null                  # null = local cluster, "ray://host:port" = remote
  dashboard_host: "127.0.0.1"    # Ray dashboard web UI host
  dashboard_port: 8265           # Ray dashboard port (http://localhost:8265)
  object_store_memory: 8000      # Shared memory cho model weights/data
  num_cpus: 20                 # null = auto-detect CPU cores
  num_gpus: 0                 # null = auto-detect GPU cards
  runtime_env: null              # Python dependencies, env vars for Ray workers
  log_level: "INFO"              # Ray internal logging (DEBUG/INFO/WARNING/ERROR)

# 3. MLflow - Model registry và tracking
mlflow:
  tracking_uri: "http://localhost:5000"     # MLflow tracking server
  registry_uri: null                        # Model registry (có thể khác tracking)
  artifact_location: null                   # Local path lưu artifacts

# =============================================================================
# PERFORMANCE & SCALING CONFIGURATIONS
# =============================================================================

# 4. Pooled Deployment - Quản lý pools cho hundreds models
pooled_deployment:
  default_pool_count: 5         # Số pools khởi tạo (2-5 pools thường đủ)
  models_per_pool: 50            # Số models tối đa mỗi pool (50-100)
  enable_cross_pool_balancing: true  # Cân bằng tải giữa pools
  pool_rebalancing_interval: 600     # Interval rebalance (10 phút)

  # Resource allocation cho mỗi pool
  pool_resource_config:
    num_cpus: 4.0                # CPU cores per pool replica
    num_gpus: 0.0                # GPU cards per pool replica
    memory: 3384                 # RAM MB per replica (8GB)
    object_store_memory: 4192    # Object store MB per replica (4GB)
    max_models_per_replica: 20   # Models max per replica instance

  # Auto-scaling cho pool replicas
  autoscaling_config:
    min_replicas: 2              # Minimum replicas always running
    max_replicas: 3             # Maximum replicas (scale for load)
    target_num_ongoing_requests_per_replica: 10 # Target concurrent requests
    metrics_interval_s: 5.0      # Metrics collection interval
    look_back_period_s: 30.0     # Historical data window for scaling
    smoothing_factor: 0.8        # Scaling sensitivity (0.1-1.0)

# 5. Tiered Loading - Chiến lược cache thông minh cho models
tiered_loading:
  enable_tiered_loading: true    # Bật tiered caching system
  hot_cache_size: 150             # "Hot" models - luôn in memory (most used)
  warm_cache_size: 200           # "Warm" models - load on demand (frequently used)
  cold_cache_size: 500           # "Cold" models - lazy load (rarely used)

  # Promotion thresholds - nâng cấp models lên tier cao hơn
  hot_promotion_threshold: 20    # Requests để promote lên Hot tier
  warm_promotion_threshold: 5    # Requests để promote lên Warm tier
  hot_promotion_time_window: 300 # Time window cho Hot promotion (5 phút)
  warm_promotion_time_window: 3600 # Time window cho Warm promotion (1 giờ)

  # Model warming - preload popular models
  enable_model_warming: true     # Bật background model warming
  warm_popular_models_count: 100 # Số models warm trước
  warming_interval: 300          # Interval check popular models (5 phút)

  # Cleanup policy
  cleanup_interval: 3600         # Interval cleanup unused models (1 giờ)
  cold_model_ttl: 7200          # TTL cho cold models (2 giờ)

# 6. Tier-Based Deployment - FIXED STRUCTURE
tier_based_deployment:
  enable_tier_based_deployment: true  # Enable tier-based deployment system

  # Dynamic scaling settings
  promotion_threshold: 20        # Requests needed to promote to higher tier
  demotion_threshold: 2          # Low requests to demote to lower tier
  promotion_time_window: 300     # Time window for promotion (5 minutes)
  demotion_time_window: 1800     # Time window for demotion (30 minutes)

  # Load balancing settings
  enable_tier_aware_routing: true     # Enable intelligent tier-aware routing
  prefer_higher_tier: true            # Prefer higher tier deployments
  routing_strategy: "least_loaded"    # FIXED: This field is now supported

  # Tier routing weights (higher = more preferred)
  tier_routing_weights:
    hot: 1.0                     # Full weight for hot tier
    warm: 0.7                    # 70% weight for warm tier
    cold: 0.3                    # 30% weight for cold tier

  # Monitoring and health checks
  tier_monitoring_interval: 30   # Tier evaluation interval (seconds)
  capacity_check_interval: 300   # Capacity monitoring interval (5 minutes)
  health_check_enabled: true     # Enable tier health monitoring

  # Business-Critical Models - Manual tier assignments
  business_critical_models:
    explicit_assignments: {}     # Direct model -> tier assignments
    patterns: {}                 # Pattern-based assignments

  # Tier-specific resource configurations - FIXED STRUCTURE
  tier_configs:
    hot:
      tier: "hot"                # FIXED: Added tier field
      num_cpus: 1           # High CPU allocation for hot models
      num_gpus: 0              # Dedicated GPU for hot models
      memory: 2096              # 12GB RAM for hot models
      object_store_memory: 2192  # 8GB object store for hot models
      max_models_per_replica: 2  # Fewer models per replica for better performance
      min_replicas: 0            # Always keep replicas running
      max_replicas: 1           # High scaling capacity
      target_requests_per_replica: 8  # FIXED: Renamed from target_num_ongoing_requests_per_replica
      priority: 100              # Highest priority

    warm:
      tier: "warm"               # FIXED: Added tier field
      num_cpus: 1.0              # Medium CPU allocation for warm models
      num_gpus: 0.0              # Shared GPU for warm models
      memory: 1192               # 8GB RAM for warm models
      object_store_memory: 4096  # 4GB object store for warm models
      max_models_per_replica: 15 # Medium models per replica
      min_replicas: 0            # Keep some replicas running
      max_replicas: 1           # Medium scaling capacity
      target_requests_per_replica: 12  # FIXED: Renamed from target_num_ongoing_requests_per_replica
      priority: 50               # Medium priority

    cold:
      tier: "cold"               # FIXED: Added tier field
      num_cpus: 1.0              # Low CPU allocation for cold models
      num_gpus: 0.0              # No GPU for cold models
      memory: 1096               # 4GB RAM for cold models
      object_store_memory: 2048  # 2GB object store for cold models
      max_models_per_replica: 30 # Many models per replica
      min_replicas: 0            # Minimal replicas
      max_replicas: 1          # Limited scaling
      target_requests_per_replica: 20  # FIXED: Renamed from target_num_ongoing_requests_per_replica
      priority: 10               # Lowest priority

# 7. Resource Sharing - Tối ưu resource utilization
resource_sharing:
  strategy: "gpu_shared"         # Strategies: none/cpu_only/gpu_shared/memory_mapped/full_sharing
  enable_memory_mapping: true    # Share model weights in memory
  enable_model_weight_sharing: true # Multiple models share same weights
  shared_memory_size: "10GB"     # Shared memory pool size

  # GPU optimization
  max_models_per_gpu: 5          # Max models per GPU card
  gpu_memory_reserve: 0.2        # Reserve 20% GPU memory cho system
  enable_dynamic_gpu_allocation: true # Dynamic GPU allocation

  # CPU optimization
  cpu_oversubscription_factor: 2.0 # Allow 2x CPU oversubscription
  enable_cpu_affinity: true       # Pin processes to CPU cores

  # Memory optimization
  enable_model_compression: true   # Compress models in memory
  compression_ratio: 0.7          # Target 70% original size
  enable_lazy_loading: true       # Load model parts on demand

# 8. Routing Configuration
routing:
  strategy: "least_loaded"       # least_loaded/round_robin/fastest_response/model_affinity
  enable_request_queuing: true   # Enable request queuing
  max_queue_size: 1000          # Maximum queue size
  queue_timeout: 30             # Queue timeout in seconds

# 9. Anomaly Detection Configuration
anomaly_detection:
#  enable_anomaly_detection: true # Enable anomaly detection
  default_probability_threshold: 0.95     # Anomaly detection threshold
  default_label_threshold: 0.0


# =============================================================================
# SYSTEM CONFIGURATIONS
# =============================================================================

# 10. Monitoring Configuration
monitoring:
  collection_interval: 30       # Metrics collection interval (seconds)
  optimization_interval: 300    # Optimization interval (seconds)
  enable_prometheus_export: true # Enable Prometheus metrics export
  prometheus_port: 9090         # Prometheus metrics port

  # Alert thresholds
  alert_thresholds:
    cpu_usage_percent: 80       # CPU usage alert threshold
    memory_usage_percent: 85    # Memory usage alert threshold
    gpu_usage_percent: 90       # GPU usage alert threshold
    response_time_ms: 5000      # Response time alert threshold
    error_rate_percent: 5       # Error rate alert threshold

# 11. Logging Configuration
logging:
  log_level: "DEBUG"
  log_dir: "logs"
  max_file_size: 104857600  # 100MB in bytes
  backup_count: 5
  enable_console: true
  enable_structured: true
  enable_performance_tracking: true

# 12. Security Configuration
security:
  enable_auth: false
  api_key: null  # Set via API_KEY environment variable
  allowed_origins: ["*"]
  rate_limit_requests_per_minute: 1000
  enable_https: false
  ssl_cert_path: null
  ssl_key_path: null


# 14. Batch Processing Configuration
batch_processing:
  enable_dynamic_batching: true
  max_batch_size: 256  # Increased for better throughput
  batch_timeout_ms: 10  # Reduced for lower latency
  adaptive_batch_sizing: true
  enable_response_caching: true
  response_cache_size: 10000
  response_cache_ttl: 300  # 5 minutes

# 15. Connection Pooling Configuration
connection_pooling:
  enable_connection_pooling: true
  max_connections: 1000
  connection_timeout: 30
  pool_connections: 10
  pool_maxsize: 20

# =============================================================================
# SYSTEM SETTINGS
# =============================================================================

# System settings
max_workers: 1                  # Increased for hundreds of models
enable_auto_deployment: true     # Enable automatic deployment
deployment_timeout: 600          # 10 minutes for large deployments

# Legacy deployment support (for backward compatibility)
deployment:
  resource_config:
    num_cpus: 0.5              # CPU cores per model replica
    num_gpus: 0.0               # GPU cards per model replica
    memory: 1024                # RAM MB per replica
    object_store_memory: 1024   # Object store MB per replica

  autoscaling:
    min_replicas: 0             # Minimum replicas
    max_replicas: 1            # Maximum replicas
    target_num_ongoing_requests_per_replica: 2 # Target requests per replica
    metrics_interval_s: 10.0    # Metrics interval
    look_back_period_s: 30.0    # Look back period
    smoothing_factor: 1.0       # Smoothing factor